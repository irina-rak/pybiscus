root_dir: ${oc.env:PWD}

trainer:
  max_epochs: 100
  # accumulate_grad_batches: 4  # Not supported by default with manual optimization (implemented for vqvaegan)
  check_val_every_n_epoch: 1  # Validate every n epochs to save time
  log_every_n_steps: 10  # Log more frequently for monitoring
  hardware:
    precision: 32-true # bf16-mixed # Use bf16-mixed instead of 16-mixed to avoind NaN values in the loss
    accelerator: gpu
    strategy: ddp_find_unused_parameters_true
    # strategy: auto
    devices:
      - 0
      - 1
      - 2
      # - 3
  metrics_loggers:
    - name: wandb
      config:
        api_key_definition:
          env_var_name: WANDB_API_KEY
        params:
          project: "Prostate_Bladder_Rectum"
          group: "pbr_gen_${model.name}"
          name: "pbr_${model.name}_local_INT_URY_e${trainer.max_epochs}_roi_${data.config.patch_size[0]}_${now:%Y%m%d_%H%M}"
          
          # project: "Decathlon"
          # group: "Task01_BrainTumour"
          # name: "BT_${model.name}_local_e${trainer.max_epochs}_${now:%Y%m%d_%H%M%S}"

          job_type: "server"
          is_client: false
  reporting_path: ${root_dir}/experiments/${trainer.metrics_loggers[0].config.params.group}/${trainer.metrics_loggers[0].config.params.name}
  # checkpointing:
  #   save_top_k: -1
  #   monitor: "val_recons_loss"
  #   mode: "min"
  #   filename: "epoch-{epoch:02d}"
  #   every_n_epochs: 1
  #   save_on_train_epoch_end: False
  checkpointing:
    save_top_k: 1
    monitor: "val_loss"          # Don't monitor any metric - save every epoch
    mode: "min"
    filename: "epoch-{epoch:02d}-{val_loss:.3f}"
    every_n_epochs: 1
    save_on_train_epoch_end: True  # Change back to True for epoch-based saving

data:
  name: pbr_gen
  config:
    dir_train: ${root_dir}/datasets/INT_URY/train_split.json
    dir_val: ${root_dir}/datasets/INT_URY/val_split.json
    dir_test: None

    # dir_train: ${root_dir}/datasets/
    # dir_val: ${root_dir}/datasets/
    # dir_test: ${root_dir}/datasets/

    task_type: "generation"  # This will use LDM preprocessing
    # patch_size: [96, 96, 96]
    # patch_size: [128, 96, 128]
    patch_size: [192, 192, 128]
    spacing: [1.0, 1.0, 2.0]
    # spacing: [2.0, 2.0, 3.0]
    margin: 10
    augment: True  # Enable augmentation for better generalization
    cache_rate: 1.0
    batch_size: 1
    num_workers: 11

model:
  name: vqvaegan
  # pretrained_weights_path: ${root_dir}/experiments/pbr_gen_vqvaegan/pbr_vqvaegan_local_INT_URY_e100_roi_192_20250802_0059_transfer/pbr_vqvaegan_local_INT_URY_e100_roi_192_20250802_0059_transfer.ckpt
  pretrained_weights_path: ${root_dir}/experiments/pbr_gen_vqvaegan/pbr_vqvaegan_local_INT_URY_e150_roi_192_20250804_1941_transfer/pbr_vqvaegan_local_INT_URY_e150_roi_192_20250804_1941_transfer.ckpt
  config:
    # compile_model: False  # Note that compilation works best on modern GPUs -- distributed operations canâ€™t benefit from speed ups at the moment
    vqvae_config:
      spatial_dims: 3
      in_channels: 1
      out_channels: 1
      channels: [128, 256, 512] # [96, 96, 192]
      num_res_layers: 3
      num_res_channels: [128, 256, 512] # [96, 96, 192]
      # downsample_parameters:
      #   - [2, 4, 1, 1]
      #   - [2, 4, 1, 1]
      #   - [2, 4, 1, 1]
      # upsample_parameters:
      #   - [2, 4, 1, 1, 0]
      #   - [2, 4, 1, 1, 0]
      #   - [2, 4, 1, 1, 0]
      num_embeddings: 512 # 32
      embedding_dim: 256 # 64
      use_checkpointing: True
    discriminator_config:
      spatial_dims: 3
      num_layers_d: 4 # 3
      channels: 64 # 32
      in_channels: ${model.config.vqvae_config.in_channels}
      out_channels: ${model.config.vqvae_config.out_channels}
      kernel_size: 4
    reconstruction_weight: 1.0
    adv_weight: 0.01
    perceptual_weight: 0.001
    jukebox_weight: 1.0
    # lr: [3e-5, 5e-4]
    lr: [1e-4, 1e-4]
    manual_accumulate_grad_batches: 4  # Manual gradient accumulation steps
    autoencoder_warm_up_n_epochs: 10  # Usually n_epochs // 10
    seed: 4294967295