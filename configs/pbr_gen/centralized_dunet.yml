root_dir: ${oc.env:PWD}

trainer:
  max_epochs: 500
  # accumulate_grad_batches: 4  # Not supported by default with manual optimization (implemented for vqvaegan)
  check_val_every_n_epoch: 1  # Validate every n epochs to save time
  log_every_n_steps: 10  # Log more frequently for monitoring
  hardware:
    precision: bf16-mixed # Use bf16-mixed instead of 16-mixed to avoind NaN values in the loss
    accelerator: gpu
    # strategy: ddp_find_unused_parameters_true
    # strategy: auto
    devices:
      # - 0
      # - 1
      # - 2
      - 3
  metrics_loggers:
    - name: wandb
      config:
        api_key_definition:
          env_var_name: WANDB_API_KEY
        params:
          project: "Prostate_Bladder_Rectum"
          group: "pbr_gen_${model.name}"
          name: "pbr_${model.name}_local_INT_URY_e${trainer.max_epochs}_roi_${data.config.patch_size[0]}_${now:%Y%m%d_%H%M}"
          
          # project: "Decathlon"
          # group: "Task01_BrainTumour"
          # name: "BT_${model.name}_local_e${trainer.max_epochs}_${now:%Y%m%d_%H%M%S}"

          job_type: "server"
          is_client: false
  reporting_path: ${root_dir}/experiments/${trainer.metrics_loggers[0].config.params.group}/${trainer.metrics_loggers[0].config.params.name}
  checkpointing:
    save_top_k: 1
    monitor: "val_loss"          # Don't monitor any metric - save every epoch
    mode: "min"
    filename: "epoch-{epoch:02d}-{val_loss:.3f}"
    every_n_epochs: 1
    save_on_train_epoch_end: True  # Change back to True for epoch-based saving

data:
  name: pbr_gen
  config:
    dir_train: ${root_dir}/datasets/INT_URY/train_split.json
    dir_val: ${root_dir}/datasets/INT_URY/val_split.json
    dir_test: None

    # dir_train: ${root_dir}/datasets/
    # dir_val: ${root_dir}/datasets/
    # dir_test: ${root_dir}/datasets/

    task_type: "generation"  # This will use LDM preprocessing
    # patch_size: [96, 96, 96]
    # patch_size: [128, 96, 128]
    patch_size: [192, 192, 128]
    spacing: [1.0, 1.0, 2.0]
    # spacing: [2.0, 2.0, 3.0]
    margin: 10
    augment: True  # Enable augmentation for better generalization
    cache_rate: 1.0
    batch_size: 1
    num_workers: 11

model:
  name: dunet
  pretrained_weights_path: ${root_dir}/experiments/pbr_gen_${model.name}/pbr_dunet_local_INT_URY_e300_roi_192_20250802_1713/pbr_dunet_local_INT_URY_e300_roi_192_20250802_1713.ckpt
  config:
    unet_config:
      spatial_dims: 3
      in_channels: 64
      out_channels: 64
      channels: [128, 256, 256]
      attention_levels: [False, False, True]
      num_res_blocks: 1
      norm_num_groups: 32
      num_head_channels: 256

    scheduler_config:
      num_train_timesteps: 1000
      schedule: "scaled_linear_beta"
      beta_start: 0.0015
      beta_end: 0.0195

    generator_config:
      spatial_dims: 3
      in_channels: 1
      out_channels: 1
      channels: [96, 96, 192]
      num_res_layers: 3
      num_res_channels: [96, 96, 192]
      # downsample_parameters:
      #   - [2, 4, 1, 1]
      #   - [2, 4, 1, 1]
      #   - [2, 4, 1, 1]
      # upsample_parameters:
      #   - [2, 4, 1, 1, 0]
      #   - [2, 4, 1, 1, 0]
      #   - [2, 4, 1, 1, 0]
      num_embeddings: 32
      embedding_dim: 64
    discriminator_config:
      spatial_dims: 3
      num_layers_d: 3
      channels: 32
      in_channels: ${model.config.generator_config.in_channels}
      out_channels: ${model.config.generator_config.out_channels}
      kernel_size: 4
    autoencoder_weights: ${root_dir}/experiments/pbr_gen_vqvaegan/pbr_vqvaegan_local_INT_URY_e100_roi_192_20250802_1306_transfer/pbr_vqvaegan_local_INT_URY_e100_roi_192_20250802_1306_transfer.ckpt
    lr: 1e-4
    seed: 4294967295